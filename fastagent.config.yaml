generic:
    api_key: "ollama"
    base_url: "http://localhost:11434/v1"

default_model: generic.llama3.2:latest
execution_engine: asyncio

logger:
    progress_display: false
    show_chat: true
    show_tools: true
    truncate_tools: true
    type: file
    level: error

mcp:
    servers:
        filesystem:
            command: "npx"
            args: ["-y", "@modelcontextprotocol/server-filesystem", "."]
        fetch:
            command: "uvx"
            args: ["mcp-server-fetch"]
