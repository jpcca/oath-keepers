{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fcddbf",
   "metadata": {},
   "source": [
    "# download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6e9c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "REPO_ROOT = subprocess.run(['git', 'rev-parse', '--show-toplevel'], capture_output=True, text=True, check=True).stdout.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381aceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOID.owl already exists. Redownload? [Y/n]: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "urls = {\n",
    "    \"DOID\": \"http://purl.obolibrary.org/obo/doid.owl\",\n",
    "    \"SYMP\": \"http://purl.obolibrary.org/obo/symp.owl\",\n",
    "    \"FMA\": \"http://purl.obolibrary.org/obo/fma.owl\",\n",
    "}\n",
    "os.makedirs(f\"{REPO_ROOT}/data/owl\", exist_ok=True)\n",
    "\n",
    "for name, url in urls.items():\n",
    "    data_path = REPO_ROOT + f\"/data/owl/{name}.owl\"\n",
    "    if os.path.exists(data_path):\n",
    "        print(f\"{name}.owl already exists. Redownload? [Y/n]: \", end=\"\")\n",
    "        if input().strip().lower() != \"y\":\n",
    "            print(f\"Skipping.\")\n",
    "            continue\n",
    "    \n",
    "    print(\"Downloading\", f\"{name}.owl ... \", end=\"\")\n",
    "    req = requests.get(url)\n",
    "    with open(data_path, \"wb\") as fw:\n",
    "        fw.write(req.content)\n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce202c",
   "metadata": {},
   "source": [
    "# read and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99323b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the maximum memory located to JVM [8g]:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: A restricted method in java.lang.System has been called\n",
      "WARNING: java.lang.System::load has been called by org.jpype.JPypeContext in an unnamed module (file:/Users/lilja/Projects/henkaku/repos/oath-keepers/.venv/lib/python3.12/site-packages/org.jpype.jar)\n",
      "WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\n",
      "WARNING: Restricted methods will be blocked in a future release unless native access is enabled\n",
      "\n",
      "INFO:deeponto:8g maximum memory allocated to JVM.\n",
      "INFO:deeponto:JVM started successfully.\n",
      "WARNING: A terminally deprecated method in sun.misc.Unsafe has been called\n",
      "WARNING: sun.misc.Unsafe::objectFieldOffset has been called by com.github.benmanes.caffeine.cache.UnsafeAccess (file:/Users/lilja/Projects/henkaku/repos/oath-keepers/.venv/lib/python3.12/site-packages/deeponto/lib/caffeine-2.8.6.jar)\n",
      "WARNING: Please consider reporting this to the maintainers of class com.github.benmanes.caffeine.cache.UnsafeAccess\n",
      "WARNING: sun.misc.Unsafe::objectFieldOffset will be removed in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(doid_list) = 14452\n",
      "len(symp_list) = 1019\n",
      "len(fma_list) = 78977\n"
     ]
    }
   ],
   "source": [
    "from deeponto.onto import Ontology\n",
    "\n",
    "# Ontology のロード\n",
    "doid_onto = Ontology(f\"{REPO_ROOT}/data/owl/DOID.owl\")\n",
    "symp_onto = Ontology(f\"{REPO_ROOT}/data/owl/SYMP.owl\")\n",
    "fma_onto = Ontology(f\"{REPO_ROOT}/data/owl/FMA.owl\")\n",
    "\n",
    "# 必要な Ontology に絞る\n",
    "doid_list = [iri for iri in doid_onto.owl_classes.keys() if iri.startswith(\"http://purl.obolibrary.org/obo/DOID_\")]\n",
    "symp_list = [iri for iri in symp_onto.owl_classes.keys() if iri.startswith(\"http://purl.obolibrary.org/obo/SYMP_\")]\n",
    "fma_list  = [iri for iri in fma_onto.owl_classes.keys() if iri.startswith(\"http://purl.obolibrary.org/obo/FMA_\")]\n",
    "print(\"len(doid_list) =\", len(doid_list))\n",
    "print(\"len(symp_list) =\", len(symp_list))\n",
    "print(\"len(fma_list) =\", len(fma_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf12f5",
   "metadata": {},
   "source": [
    "# save master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6bc5c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOID_master.tsv saved.\n",
      "SYMP_master.tsv saved.\n",
      "FMA_master.tsv saved.\n"
     ]
    }
   ],
   "source": [
    "from org.semanticweb.owlapi.vocab import OWLRDFVocabulary\n",
    "\n",
    "# アノテーションプロパティの IRI\n",
    "class OwlIri:\n",
    "    RDFS_LABEL = \"http://www.w3.org/2000/01/rdf-schema#label\"  # label = その IRI の名前を表す IRI\n",
    "    OBO_ID = \"http://www.geneontology.org/formats/oboInOwl#id\" # OBO ID = その IRI の短縮 ID を表す IRI\n",
    "    IAO_DEF = \"http://purl.obolibrary.org/obo/IAO_0000115\"  # IAO = その IRI の定義を表す IRI\n",
    "    SYN_EXACT = \"http://www.geneontology.org/formats/oboInOwl#hasExactSynonym\"  # 同義語 (厳密)\n",
    "    SYN_RELATED = \"http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym\"  # 関連語\n",
    "    RO_SYMPTOM  = \"RO_0002452\"  # this means `doid has_symptom symp` https://www.ebi.ac.uk/ols4/ontologies/ro/properties/http%253A%252F%252Fpurl.obolibrary.org%252Fobo%252FRO_0002452\n",
    "\n",
    "\n",
    "def get_ontology_info(onto: Ontology, iri: str):\n",
    "    \"\"\"\n",
    "    Ontology インスタンスと IRI から、その IRI の短縮 ID、名前、定義を取得する。\n",
    "\n",
    "    Arguments:\n",
    "        onto: owl ファイルをロードした Ontology インスタンス\n",
    "        iri: \"http://purl.obolibrary.org/obo/DOID_0002116\" のような完全 IRI\n",
    "\n",
    "    Returns:\n",
    "        (short_id, name, definition)\n",
    "    \"\"\"\n",
    "    obj = onto.get_owl_object(iri)\n",
    "    properties = onto.owl_annotation_properties\n",
    "\n",
    "    if OwlIri.OBO_ID in properties:\n",
    "        id_annots = onto.get_annotations(obj, annotation_property_iri=OwlIri.OBO_ID)\n",
    "        short_id = next(iter(id_annots), None)\n",
    "    else:\n",
    "        short_id = None\n",
    "\n",
    "    if OwlIri.RDFS_LABEL in properties:\n",
    "        name_annots = onto.get_annotations(obj, annotation_property_iri=OwlIri.RDFS_LABEL)\n",
    "        name = next(iter(name_annots), None)\n",
    "    else:\n",
    "        name = None\n",
    "\n",
    "    if OwlIri.SYN_EXACT in properties:\n",
    "        synonym_exact_annots = onto.get_annotations(obj, annotation_property_iri=OwlIri.SYN_EXACT)\n",
    "        synonym_exact = next(iter(synonym_exact_annots), None)\n",
    "    else:\n",
    "        synonym_exact = None\n",
    "\n",
    "    if OwlIri.SYN_RELATED in properties:\n",
    "        synonym_related_annots = onto.get_annotations(obj, annotation_property_iri=OwlIri.SYN_RELATED)\n",
    "        synonym_related = next(iter(synonym_related_annots), None)\n",
    "    else:\n",
    "        synonym_related = None\n",
    "\n",
    "    if OwlIri.IAO_DEF in properties:\n",
    "        def_annots = onto.get_annotations(obj, annotation_property_iri=OwlIri.IAO_DEF)\n",
    "        definition = next(iter(def_annots), None)\n",
    "    else:\n",
    "        definition = None\n",
    "\n",
    "    return (iri, short_id, name, synonym_exact, synonym_related, definition)\n",
    "\n",
    "\n",
    "doid_master = [get_ontology_info(doid_onto, doid_iri) for doid_iri in doid_list]\n",
    "symp_master = [get_ontology_info(symp_onto, symp_iri) for symp_iri in symp_list]\n",
    "fma_master  = [get_ontology_info(fma_onto,  fma_iri)  for fma_iri  in fma_list]\n",
    "\n",
    "# 出力\n",
    "for onto_name, master in [(\"DOID\", doid_master), (\"SYMP\", symp_master), (\"FMA\", fma_master)]:\n",
    "    with open(f\"{REPO_ROOT}/data/master/{onto_name}.tsv\", \"w\") as fw:\n",
    "        fw.write(\"iri\\tid\\tname\\texact_synonym\\trelated_synonym\\tdefinition\\n\")\n",
    "        for iri, short_id, name, synonym_exact, synonym_related, definition in master:\n",
    "            fw.write(f\"{iri}\\t{short_id}\\t{name}\\t{synonym_exact}\\t{synonym_related}\\t{definition}\\n\")\n",
    "\n",
    "    print(f\"{onto_name}_master.tsv saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "28e3d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_symp = pd.read_table(f\"{REPO_ROOT}/data/master/SYMP.tsv\", sep=\"\\t\", header=0, dtype=str)\n",
    "df_doid = pd.read_table(f\"{REPO_ROOT}/data/master/DOID.tsv\", sep=\"\\t\", header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be6325",
   "metadata": {},
   "source": [
    "# desease and symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a79b5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from rapidfuzz.process import cdist\n",
    "from org.semanticweb.owlapi.model import AxiomType, OWLObjectSomeValuesFrom\n",
    "\n",
    "def doid2symp_by_axiom(doid_onto: Ontology):\n",
    "    \"\"\"\n",
    "    RO_0002452(has_symp) を使った axiom ベースのマッチング\n",
    "    ontology の axiom では `doid has_symptom symp` という形式の axiom しか存在しないため\n",
    "    doid から symptom を引く。\n",
    "    e.g.\n",
    "        Malaria ⊑ Disease ⊓ has_symptom some Fever\n",
    "\n",
    "    Arguments:\n",
    "        doid_onto:  Ontology instance for DOID.owl\n",
    "    Returns:\n",
    "        Dict of doid_iri -> List of tuples (symptom_iri, method, score)\n",
    "        where method is \"axiom\" and score is 1.0\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for doid_iri in tqdm.tqdm(doid_list):\n",
    "        for sup in doid_onto.get_asserted_parents(doid_obj, named_only=False):\n",
    "            if isinstance(sup, OWLObjectSomeValuesFrom):\n",
    "                prop = sup.getProperty()\n",
    "                if OwlIri.RO_SYMPTOM in str(prop):\n",
    "                    filler = sup.getFiller()\n",
    "                    symp_iri = str(filler.getIRI())\n",
    "                    result.append([doid_iri, symp_iri, \"axiom\", 1.0])\n",
    "        \n",
    "    return pd.DataFrame(result, columns=[\"doid_iri\", \"symptom_iri\", \"method\", \"score\"])\n",
    "\n",
    "def symp2doid_by_keyword(df_symp: DataFrame, df_doid: DataFrame, topk=5):\n",
    "    \"\"\"\n",
    "    symp と doid の名前、定義、同義語を使ったキーワードベースのマッチング\n",
    "    e.g.\n",
    "        DOID:0050117  Malaria  hasExactSynonym: \"malaria\"\n",
    "        SYMP:0000185 Fever   name: \"fever\", hasExactSynonym: \"pyrexia\"\n",
    "\n",
    "    Arguments:\n",
    "        df_symp: DataFrame for SYMP.tsv\n",
    "        df_doid: DataFrame for DOID.tsv\n",
    "        topk: Return top k results for each doid\n",
    "    Returns:\n",
    "        Dict of doid_iri -> List of tuples (symptom_iri, method)\n",
    "        where method is \"synonym\"\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    _df_symp = df_symp.copy().fillna(\"\")\n",
    "    _df_doid = df_doid.copy().fillna(\"\")\n",
    "\n",
    "    def _join_keywords(row):\n",
    "        return f\"name={row['name']}:exact_synonym={row['exact_synonym']}:related_synonym={row['related_synonym']}:definition={row['definition']}\"\n",
    "\n",
    "    _df_symp['keyword'] = _df_symp[['name', 'exact_synonym', 'related_synonym', 'definition']].apply(_join_keywords, axis=1).str.lower()\n",
    "    _df_doid['keyword'] = _df_doid[['name', 'exact_synonym', 'related_synonym', 'definition']].apply(_join_keywords, axis=1).str.lower()\n",
    "\n",
    "    queries = _df_symp['keyword'].tolist()\n",
    "    choices = _df_doid['keyword'].tolist()\n",
    "\n",
    "    scores = cdist(queries, choices, workers=-1)\n",
    "    topk_indices = np.argsort(scores, axis=1)[:, -topk:][:, ::-1]\n",
    "    rows = np.arange(scores.shape[0])[:, None]\n",
    "    topk_doid_iris = [_df_doid.iloc[topk_index]['iri'].tolist() for topk_index in topk_indices]  # shape: (num_symptoms, topk)\n",
    "    topk_confidences = scores[rows, topk_indices].tolist()  # shape: (num_symptoms, topk)\n",
    "\n",
    "    for symp_iri, topk_doid_iri, topk_confidence in zip(_df_symp['iri'], topk_doid_iris, topk_confidences):\n",
    "        for doid_iri, confidence in zip(topk_doid_iri, topk_confidence):\n",
    "            results.append([doid_iri, symp_iri, \"keyword\", confidence])\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"doid_iri\", \"symptom_iri\", \"method\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0d2aad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14452/14452 [00:00<00:00, 22516.37it/s]\n"
     ]
    }
   ],
   "source": [
    "df_results_axiom = doid2symp_by_axiom(doid_onto)\n",
    "(\n",
    "    df_results_axiom\n",
    "    .set_index(\"doid_iri\")\n",
    "    .join(\n",
    "        df_doid.set_index(\"iri\")[\"name\"].rename(\"doid_name\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .set_index(\"symptom_iri\")\n",
    "    .join(\n",
    "        df_symp.set_index(\"iri\")[[\"name\", \"definition\"]].rename({\"name\": \"symptom_name\", \"definition\": \"symptom_definition\"}, axis=1)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename({\"index\": \"symptom_iri\"}, axis=1)\n",
    ").to_csv(f\"{REPO_ROOT}/data/relationship/symp2doid_by_axiom.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae142fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_keyword = symp2doid_by_keyword(df_symp, df_doid, topk=5)\n",
    "(\n",
    "    df_results_keyword\n",
    "    .set_index(\"doid_iri\")\n",
    "    .join(\n",
    "        df_doid.set_index(\"iri\")[\"name\"].rename(\"doid_name\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .set_index(\"symptom_iri\")\n",
    "    .join(\n",
    "        df_symp.set_index(\"iri\")[[\"name\", \"definition\"]].rename({\"name\": \"symptom_name\", \"definition\": \"symptom_definition\"}, axis=1)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename({\"index\": \"symptom_iri\"}, axis=1)\n",
    ").to_csv(f\"{REPO_ROOT}/data/relationship/symp2doid_by_keyword.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0758071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5556a991",
   "metadata": {},
   "source": [
    "# symptom and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b39593ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_fma = pd.read_table(f\"{REPO_ROOT}/data/master/FMA.tsv\", sep=\"\\t\", header=0, dtype=str)\n",
    "fma_labels = {row['iri']: row['name'] for _, row in df_fma.iterrows()}\n",
    "\n",
    "df_symp = pd.read_table(f\"{REPO_ROOT}/data/master/SYMP.tsv\", sep=\"\\t\", header=0, dtype=str)\n",
    "symp_labels = {row['iri']: row['name'] for _, row in df_symp.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d53f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# キーワードマッチで症状と身体の部位のマッチング\n",
    "from rapidfuzz.process import cdist\n",
    "\n",
    "df_symp_valid = df_symp[~df_symp.definition.isna() | ~df_symp.name.isna()].copy()\n",
    "queries = df_symp_valid.name + \": \" + df_symp_valid.definition\n",
    "choices = df_fma.name\n",
    "\n",
    "score = cdist(queries, choices, workers=-1)\n",
    "best_matched_body = df_fma.iloc[pd.DataFrame(score, index=queries, columns=choices).values.argmax(axis=1)].iri\n",
    "\n",
    "with open(f\"{REPO_ROOT}/data/relationship/symp_fma.tsv\", \"w\") as fw:\n",
    "    print(\"symptom_iri\\tbody_part_iri\\tmethod\", file=fw)\n",
    "    for symp_iri, body_iri in zip(df_symp_valid.iri, best_matched_body):\n",
    "        print(f\"{symp_iri}\\t{body_iri}\\tkeyword\", file=fw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oath-keepers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
